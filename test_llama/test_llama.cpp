// Prevent Windows.h from defining min/max macros
#define NOMINMAX

#include <iostream>
#include <string>
#include <fstream>
#include <filesystem>
#include <vector>
#include <chrono>
#include <iomanip>
#include <sstream>
#include <algorithm> // For std::max

// For Windows-specific performance counter
#ifdef _WIN32
#include <windows.h>
#endif

// Forward declarations to avoid including full headers
namespace ai_editor {

enum class MessageRole {
    SYSTEM,
    USER,
    ASSISTANT,
    TOOL,
    FUNCTION
};

struct Message {
    MessageRole role;
    std::string content;
    std::string name;  // For function/tool roles
    
    Message(MessageRole r, const std::string& c) : role(r), content(c) {}
    Message(MessageRole r, const std::string& c, const std::string& n) 
        : role(r), content(c), name(n) {}
};

class LlamaProvider {
public:
    explicit LlamaProvider(const std::string& modelPath) : modelPath_(modelPath) {}
    
    bool initialize() {
        std::cout << "Initializing LlamaProvider..." << std::endl;
        
        // Check if model file exists
        if (!std::filesystem::exists(modelPath_)) {
            std::cerr << "Error: Model file not found at " << modelPath_ << std::endl;
            return false;
        }
        
        // Check file size
        auto fileSize = std::filesystem::file_size(modelPath_);
        std::cout << "Model file size: " << (fileSize / (1024 * 1024)) << " MB" << std::endl;
        
        if (fileSize < 1024 * 1024) {  // Less than 1MB
            std::cerr << "Error: Model file is too small (" << fileSize << " bytes)" << std::endl;
            return false;
        }
        
        std::cout << "Model path: " << modelPath_ << std::endl;
        std::cout << "Initialization successful" << std::endl;
        return true;
    }
    
    std::string generateResponse(const std::string& prompt, int max_tokens = 100) {
        std::cout << "\nGenerating response..." << std::endl;
        
        // Simulate some processing time
        #ifdef _WIN32
            LARGE_INTEGER frequency, start, end;
            QueryPerformanceFrequency(&frequency);
            QueryPerformanceCounter(&start);
        #else
            auto start = std::chrono::high_resolution_clock::now();
        #endif
        
        // Simulate different responses based on input
        std::string response;
        if (prompt.find("hello") != std::string::npos) {
            response = "Hello! How can I assist you today?";
        } 
        else if (prompt.find("code") != std::string::npos) {
            response = "Here's a simple function in C++ that demonstrates a basic concept.\n\n"
                     "```cpp\n"
                     "#include <iostream>\n"
                     "#include <vector>\n\n"
                     "int main() {\n"
                     "    std::vector<int> numbers = {1, 2, 3, 4, 5};\n"
                     "    int sum = 0;\n\n"
                     "    for (int num : numbers) {\n"
                     "        sum += num;\n"
                     "    }\n\n"
                     "    std::cout << \"The sum is: \" << sum << std::endl;\n"
                     "    return 0;\n"
                     "}\n"
                     "```";
        }
        else if (prompt.find("math") != std::string::npos) {
            response = "Let's solve a math problem! The square root of 144 is 12.";
        }
        else {
            response = "I'm a simulated Llama model. You asked: \"" + prompt + "\"\n"
                     "This is a test response. In a real implementation, this would be generated by the model.";
        }
        
        // Limit response length if needed
        size_t max_length = static_cast<size_t>(std::max(1, max_tokens * 4));
        if (response.length() > max_length) {
            response = response.substr(0, max_length) + "...";
        }
        
        #ifdef _WIN32
            QueryPerformanceCounter(&end);
            double duration = (end.QuadPart - start.QuadPart) * 1000.0 / frequency.QuadPart;
        #else
            auto end = std::chrono::high_resolution_clock::now();
            double duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
        #endif
        
        std::stringstream ss;
        ss << "\n=== Response (" << std::fixed << std::setprecision(2) << duration << " ms) ===\n"
           << response << "\n"
           << "=======================\n";
        
        return ss.str();
    }
    
private:
    std::string modelPath_;
};

} // namespace ai_editor

// Helper function to run a test case
bool runTest(ai_editor::LlamaProvider& provider, const std::string& name, const std::string& prompt, 
             const std::string& expectedSubstring = "") {
    std::cout << "\n[TEST] " << name << "..." << std::endl;
    std::cout << "Prompt: " << (prompt.length() > 50 ? prompt.substr(0, 47) + "..." : prompt) << std::endl;
    
    try {
        std::string response = provider.generateResponse(prompt);
        std::cout << response << std::endl;
        
        if (!expectedSubstring.empty() && 
            response.find(expectedSubstring) == std::string::npos) {
            std::cerr << "[TEST FAILED] Expected substring not found in response" << std::endl;
            return false;
        }
        return true;
    } catch (const std::exception& e) {
        std::cerr << "[TEST FAILED] Exception: " << e.what() << std::endl;
        return false;
    } catch (...) {
        std::cerr << "[TEST FAILED] Unknown error" << std::endl;
        return false;
    }
}

// Function to run all tests
void runAllTests(ai_editor::LlamaProvider& provider) {
    std::cout << "\n=== Running Test Suite ===" << std::endl;
    
    std::vector<std::tuple<std::string, std::string, std::string>> tests = {
        {"Greeting Test", "Hello, how are you?", "Hello"},
        {"Code Generation Test", "Write a C++ function to calculate factorial", "int"},
        {"Math Question Test", "What is 123 * 45?", "5535"},
        {"General Knowledge Test", "Who was the first president of the United States?", "Washington"}
    };
    
    int passed = 0;
    for (const auto& [name, prompt, expected] : tests) {
        if (runTest(provider, name, prompt, expected)) {
            passed++;
        }
    }
    
    std::cout << "\n=== Test Results ===" << std::endl;
    std::cout << "Passed: " << passed << " / " << tests.size() << std::endl;
    std::cout << "===================" << std::endl;
}

void printHelp() {
    std::cout << "\n=== Llama Model Test Environment ===" << std::endl;
    std::cout << "Available Commands:" << std::endl;
    std::cout << "  help       - Show this help message" << std::endl;
    std::cout << "  test       - Run automated test suite" << std::endl;
    std::cout << "  benchmark  - Run performance benchmark" << std::endl;
    std::cout << "  version    - Show version information" << std::endl;
    std::cout << "  clear      - Clear the screen" << std::endl;
    std::cout << "  quit/exit  - Exit the program" << std::endl;
    std::cout << "\nEnter any other text to send as a prompt to the model." << std::endl;
}

int main(int argc, char* argv[]) {
    try {
        std::string modelPath = "stable-code-3b.Q4_K_M.gguf";
        
        // If a model path is provided as a command-line argument, use it
        if (argc > 1) {
            modelPath = argv[1];
        }
        
        // Convert to absolute path
        std::filesystem::path absPath = std::filesystem::absolute(modelPath);
        modelPath = absPath.string();
        
        std::cout << "=== Llama Model Test ===" << std::endl;
        std::cout << "Looking for model at: " << modelPath << std::endl;
        
        ai_editor::LlamaProvider provider(modelPath);
        
        std::cout << "Initializing LlamaProvider..." << std::endl;
        if (!provider.initialize()) {
            std::cerr << "\nError: Failed to initialize LlamaProvider" << std::endl;
            std::cerr << "Please check that the model file exists and is accessible." << std::endl;
            return 1;
        }
        
        std::cout << "\n=== Llama Model Test Environment ===" << std::endl;
        std::cout << "Model: stable-code-3b.Q4_K_M.gguf" << std::endl;
        std::cout << "Type 'help' for a list of commands" << std::endl;
        std::cout << "Type 'test' to run automated tests" << std::endl;
        std::cout << "Type 'quit' or 'exit' to quit\n" << std::endl;
        
        // Run a quick test automatically
        std::cout << "Running quick test..." << std::endl;
        std::string testResponse = provider.generateResponse("Hello, are you working?");
        std::cout << testResponse << std::endl;
        std::cout << "\nTest completed. You can now enter your own prompts." << std::endl;
        
        std::string input;
        while (true) {
            std::cout << "\n> ";
            if (!std::getline(std::cin, input)) {
                // Handle Ctrl+Z or other input errors
                if (std::cin.eof()) {
                    std::cout << "\nExiting..." << std::endl;
                    break;
                }
                std::cin.clear();
                std::cin.ignore(std::numeric_limits<std::streamsize>::max(), '\n');
                std::cerr << "Error reading input. Please try again." << std::endl;
                continue;
            }
            
            // Trim whitespace from input
            input.erase(0, input.find_first_not_of(" \t\n\r\f\v"));
            input.erase(input.find_last_not_of(" \t\n\r\f\v") + 1);
            
            if (input.empty()) {
                continue;  // Skip empty input
            }
            
            // Handle commands
            if (input == "quit" || input == "exit") {
                std::cout << "Exiting..." << std::endl;
                break;
            } else if (input == "help") {
                printHelp();
            } else if (input == "version") {
                std::cout << "\n=== Version Information ===" << std::endl;
                std::cout << "Llama Model Test v1.0.0" << std::endl;
                std::cout << "Model: stable-code-3b.Q4_K_M.gguf" << std::endl;
                std::cout << "Build: " << __DATE__ << " " << __TIME__ << std::endl;
            } else if (input == "test") {
                runAllTests(provider);
            } else if (input == "clear" || input == "cls") {
                #ifdef _WIN32
                    system("cls");
                #else
                    system("clear");
                #endif
            } else if (input == "benchmark") {
                std::cout << "\n=== Running Benchmark ===" << std::endl;
                std::vector<std::string> benchmarkPrompts = {
                    "Hello, how are you?",
                    "Write a short poem about programming",
                    "Explain quantum computing in simple terms",
                    "What is the meaning of life?"
                };
                
                for (const auto& prompt : benchmarkPrompts) {
                    auto start = std::chrono::high_resolution_clock::now();
                    std::string response = provider.generateResponse(prompt);
                    auto end = std::chrono::high_resolution_clock::now();
                    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
                    
                    std::cout << "\n[Prompt] " << prompt << std::endl;
                    std::cout << "[Time] " << duration.count() << " ms" << std::endl;
                    std::cout << "[Response Length] " << response.length() << " characters" << std::endl;
                }
                std::cout << "\nBenchmark completed." << std::endl;
            } else {
                try {
                    std::string response = provider.generateResponse(input);
                    std::cout << response << std::endl;
                } catch (const std::exception& e) {
                    std::cerr << "\nError generating response: " << e.what() << std::endl;
                } catch (...) {
                    std::cerr << "\nUnknown error occurred while generating response" << std::endl;
                }
            }
        }
        
    } catch (const std::exception& e) {
        std::cerr << "Error: " << e.what() << std::endl;
        return 1;
    }
    
    return 0;
}
